---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Sarah Zhang (sz6753)

### Introduction 

The dataset that I chose is the Mroz dataset which I acquired from https://vincentarelbundock.github.io/Rdatasets/datasets.html. This dataset shows data about the U.S. Women's Labor-Force Participation which interested me because I'm a woman who is planning on participating in the labor-force and I'm curious to see how/if some factors affect women's labor-force participation. This dataset has 753 observations and 8 variables. The variables are lfp (binary), k5 (numeric), k618 (numeric), age (numeric), wc (binary), hc (binary), lwg (numeric), and inc (numeric). The lfp variable represents a boolean value of whether the woman participates in the labor-force or not. The k5 variable represents the number of children that are 5 years old or younger that the woman has. The k618 variable represents the number of children that are 6-17 years old that the woman has. The age variable represents the age of the woman. The wc variable represents if the woman attended college or not. The hc variable represents if the woman's husband attended college or not. The lwg represents the log expected wage rate of the woman. For women in the labor force, it represents the actual wage rate and for women not in the labor force it represents an imputed value based on the regression of lwg on the other variables. The inc variable represents the income of the woman's family exclusive of wife's income. None of the variables are missing data so there are also 753 observations for each of the variables.

```{R}
library(tidyverse)
mroz <- read_csv('Mroz.csv')
mroz <- mroz %>% select(-X1)
head(mroz)
```

### Cluster Analysis

```{R}
library(cluster)
library(GGally)
sil_width<-vector() 

mroz_numeric <- mroz %>% select(-wc)  %>% select(-hc) %>% select(-lfp) 
for(i in 2:10){
  kms <- kmeans(mroz_numeric,centers=i)
  sil <- silhouette(kms$cluster,dist(mroz_numeric))
  sil_width[i]<-mean(sil[,3])
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

pam1 <- mroz_numeric %>% pam(k=2)
pam1

mroz_numeric %>% mutate(cluster=as.factor(pam1$clustering)) %>% 
ggpairs(columns = 1:5, aes(color=cluster))

pam1$silinfo$avg.width
pam1$silinfo$clus.avg.widths
```
```{R}
plot(pam1, which=2)
pam1$silinfo$avg.width
pam1$silinfo$clus.avg.widths
```
I performed PAM clustering on the numeric variables, k5, k618, age, lwg, and inc, of the mroz dataset. From the plot, there seems to be a lot of overlap of the two clusters with the k5, k618, and lwg variables. The age variable also has a lot of overlap but the second cluster is just slightly higher than the first one. The inc variable has the least overlap between the two clusters with the second cluster a bit higher than the first one. Looking at the comparisons between each variable, there doesn't seem to be much of a correlation between them except the inc variable. The scatterplots show both cluster points scattered throughout the plot with no specific pattern or strong relationship and the correlations are also close to 0. Looking at the scattterplots of the inc variable with every other variable, the second cluster points all seem to be higher in income than the first cluster points. The average silhouette width is between 0.26 and 0.5 which indicates that the cluster solution has a weak structure and could be artificial but it is still acceptable. However, the cluster structure might just be noise.
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
mroz_numeric_scaled <- mroz_numeric %>% select_if(is.numeric) %>% scale
mroz_scaled_pca <- princomp(mroz_numeric_scaled)
summary(mroz_scaled_pca, loadings=T)

# determine how many PCs to keep
eigval <- mroz_scaled_pca$sdev^2
round(cumsum(eigval)/sum(eigval), 2) 

# plot PCA scores
mrozdf <- data.frame(PC1=mroz_scaled_pca$scores[, 1], PC2=mroz_scaled_pca$scores[, 2])
ggplot(mrozdf, aes(PC1, PC2)) + geom_point()

mrozdf <- data.frame(PC1=mroz_scaled_pca$scores[, 1], PC3=mroz_scaled_pca$scores[, 3])
ggplot(mrozdf, aes(PC1, PC3)) + geom_point()

mrozdf <- data.frame(PC2=mroz_scaled_pca$scores[, 2], PC3=mroz_scaled_pca$scores[, 3])
ggplot(mrozdf, aes(PC2, PC3)) + geom_point()
```

I performed PCA on all of the numeric variables from the dataset. I decided to keep the first three PCs because they were the only PCs that had a cumulative proportion of variance less than 80% which follows the rule of thumb to "Pick PCs until cumulative proportion of variance is > 80%". PC1 is a k5/k618 vs age axis since Higher scores on PC1 means higher number of children under 18 years old but lower in age of the woman and lower scores on PC1 scores means lower number of children under 18 years old but higher in age of the woman. PC2 is a lwg and inc axis since they have a similar magnitude and the same sign. Higher scores on PC2 means higher log expected wage rate and higher family income and lower scores on PC2 means lower log expected wage rate and lower family income. PC3 is a k5/lwg vs k618/inc axis since they have similar magnitudes but opposite signs. Higher scores on PC3 means higher number of children under 5 years old and higher log expected wage rate but lower number of children 6-18 years old and lower family income and lower scores on PC3 means lower number of children under 5 years old and lower log expected wage rate but higher number of children 6-18 years old and higher family income.

I also plotted the first three PC scores with each other and there doesn't seem to be any correlation or strong relationship among them. The points for the PC1 and PC2 scatterplot are mainly clustered randomly horizontally in the middle with some extreme points along the top, right, and bottom of the plot. The points for the PC1 and PC2 scatterplot are also mainly clustered randomly horizontally in the middle and left side with some extreme points along the top, right, and bottom of the plot. The points for the PC2 and PC3 scatterplot are mainly clustered right in the center of the plot with extreme points all along the sides of the plot. About 74% of the total variance in the dataset is explained by these PCs (first three PCs).
 

###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- glm(lfp=="TRUE" ~ k5 + k618 + age + lwg + inc, data=mroz, family="binomial")

pred <- predict(logistic_fit, type="response")

class_diag(pred, truth=mroz$lfp, positive="TRUE")

y <- mroz$lfp
table(truth = y, prediction = pred>.5)
```

```{R}
# cross-validation of linear classifier here
set.seed(1234)
k=10

data <- mroz[sample(nrow(mroz)),]
folds <- cut(seq(1:nrow(mroz)), breaks=k, labels=F) 

diags <- NULL

for(i in 1:k){
  
  train <- data[folds!=i,]
  test <- data[folds==i,]
  truth <- test$lfp 
  
  fit <- glm(lfp=="TRUE" ~ k5 + k618 + age + lwg + inc, data=train, family="binomial")
  
  probs <- predict(fit, newdata = test, type="response")
  
  diags <- rbind(diags, class_diag(probs, truth, positive="TRUE"))
}

summarize_all(diags,mean)

```

I first performed a logistic regression on the numerical variables, k5, k618, age, lwg, and inc, of the mroz dataset with lfp (labor force participation) as the binary variable to be predicted. The AUC for the logisitic regression model came out to be 0.7307. The AUC (area under curve) quantifies how well we are predicting overall. The AUC for this model isn't that high which means that the performance of the model is fair. I also created a confusion matrix of the predicted labor force participation positives/negatives against the actual positives/negatives for the full model. The confusion matrix shows that there were 343 true positives, 85 false negatives, 156 true negatives, and 169 false positives. Then, I performed a k-fold cross validation with 10 folds on the data set with the same variables. The AUC for the k-fold CV came out to be 0.7202 which is a decrease from the AUC of the logistic regression model which is a sign of overfitting. This is also shows that the logistic regression model performed the best on the data since its AUC is higher with an AUC of 0.7307 vs an AUC of 0.7202.

### Non-Parametric Classifier

```{R}
library(caret)

knn_fit <- knn3(lfp=="TRUE" ~ k5 + k618 + age + lwg + inc, data=mroz)

pred <- predict(knn_fit, mroz)

class_diag(pred[,2], mroz$lfp, positive="TRUE")

# confusion matrix
y <- mroz$lfp
table(truth = y, prediction = pred[,2]>.5)
```

```{R}
# cross-validation of np classifier here
set.seed(1234)
k = 10

data <- mroz[sample(nrow(mroz)),]
folds <- cut(seq(1:nrow(mroz)), breaks=k, labels=F) 

diags <- NULL

for(i in 1:k){
  train <- data[folds!=i,]
  test <- data[folds==i,]
  truth <- test$lfp 
  
  fit <- knn3(lfp=="TRUE" ~ k5 + k618 + age + lwg + inc, data=train)
  
  probs <- predict(fit, newdata = test)[,2]
  
  diags <- rbind(diags, class_diag(probs, truth, positive="TRUE"))
}

summarize_all(diags,mean)

```

Here, I performed k-nearest-neighbor as my non-parametric classifier on all the numeric variables, k5, k618, age, lwg, and inc, of the mrox dataset with labor force participation (lfp) as the response variable. The AUC for this model came out to be 0.7822 which also isn't very high which means that the performance of the model is fair. I then created a confusion matrix of the predicted labor force participation positives/negatives against the actual positives/negatives for the full model. The confusion matrix shows that there were 329 true positives, 99 false negatives, 219 true negatives, and 106 false positives. Then, I performed a k-fold cross validation on the model with 10 folds. The AUC for the k-fold CV came out to be 0.56077 which is a clear decrease form the knn model which is a sign of overfitting. This is also shows that the knn model performed better on the data than the k-fold CV since its AUC is much higher with an AUC of 0.7822 vs an AUC of 0.56077. The nonparametric model (knn) performed worse than the linear model (logistic regression) in its cross-validation performance since the AUC is much lower and had a greater decrease. The AUC of the k-fold CV on the nonparametric model came out to be 0.56077 which is very low and indicates that the model performed badly whereas the AUC of the k-fold CV on the linear model came out to be 0.7195 which isn't great either but at least performs fairly. 



### Regression/Numeric Prediction

```{R}
# regression model code here
linear_fit <- lm(inc ~ k5 + k618 + age + lwg, data=mroz) 
pred <- predict(linear_fit)

mean((mroz$inc-pred)^2)
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5
data <- mroz[sample(nrow(mroz)),]
folds <- cut(seq(1:nrow(mroz)), breaks=k, labels=F) 

diags <- NULL
for(i in 1:k){
  train <- data[folds!=i,]
  test <- data[folds==i,]
  
  fit <- lm(inc ~ k5 + k618 + age + lwg, data=train)
 
  pred <- predict(fit, newdata=test)
  
  diags <- mean((test$inc-pred)^2)
}
mean(diags)

```

I performed a linear regression on the mroz dataset, predicting income (inc) from all the other numeric variables, k5, k618, age, and lwg. I calculated the MSE (mean squared error) for the overall dataset which came out to be 131.27. Then, I performed a k-fold cross validation with the same model and same variables using 5 folds. The average MSE across the 5 testing folds came out to be 163.9256 which is much higher than the linear regression MSE which is a sign of overfitting.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)
mroz_children <- mroz[2:3]
head(mroz_children)
```

```{python}
# python code here
import pandas as pd
data = r.mroz_children
df = pd.DataFrame.from_dict(data)
df['total_children'] = df['k5'] + df['k618']
df.head()
```
```{R}
df <- py$df
mroz["total_children"] <- df['total_children']
head(mroz)
```

In the first R chunk, I created a new dataset off of the original mroz dataset to only include the k5 and k618 columns which represents the number of children under 5 years old and number of children 6-17 years old, respectively. In the python code chunk, I used 'r.' to get the new R dataset object that I created and converted it to a Pandas dataframe so that I could manipulate it. I added a new column called "total_children" that represented the sum of the number of children under 18. Finally, in the last R code chunk, I used 'py$' to get the pandas dataframe with the new column "total_children" and added that column to the original mroz dataset.

### Concluding Remarks

Overall, I thought it was very interesting to look over this dataset and its variables. As a woman who plans on participating in the labor force, I was curious to see the different factors that could affect labor force participation of women. It was also interesting to see the performance of the different models that I created and trained on. All my models had a fair performance which isn't great but still cool to see that some of the variables, such as k5, k618, age, lwg, and inc, were related enough to make some accurate predictions. I also enjoyed bringing all our class concepts together in this project to see how I can fully analyze data.




